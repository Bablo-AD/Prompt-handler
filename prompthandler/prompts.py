from .models import openai_chat_gpt
import tiktoken

class PromptHandler(openai_chat_gpt):
    """
    This class represents the prompt  history for a conversation.
    """

    def __init__(self,MAX_TOKEN=4096,api_key=None,temperature=0,model="gpt-3.5-turbo-0613"):
        # Initializes the message history.
        self.headers = []
        self.body = []
        self.head_tokens = 0
        self.body_tokens = 0
        self.tokens = 0
        
                
        if 'gpt' in model:
            super().__init__(MAX_TOKEN=MAX_TOKEN,api_key=api_key,temperature=temperature,model=model)
        self.update_messages()


    def get_completion(self, message='', update_history=True, temperature=None):

        """
        Generates a completion for the conversation history.

        Args:
            message (str): The user's message to be added to the history.
            update_history (bool): Flag to update the conversation history.
            temperature (float): Control the randomness of the output.

        Returns:
            str: The completion generated by the model.
            int: The number of tokens used by the completion.
        """
        self.update_messages()
        if message == '':
            self.calibrate()
            completion_output = self.get_completion_for_message(self.messages, temperature)
            if update_history:
                self.add_assistant(completion_output[0])
                return self.get_last_message()['content']
            else:
                return completion_output
        else:
            if update_history:
                self.add_user(message)
                self.calibrate()
                completion_output = self.get_completion_for_message(self.messages, temperature)
                self.add_assistant(completion_output[0])
                return self.get_last_message()['content']
            else:
                temporary_message = self.messages.copy()
                temporary_message.append({"role": "user", "content": message})
                completion_output = self.get_completion_for_message(temporary_message, temperature)
                return completion_output

    def chat(self, update_history=True, temperature=None):
        """
        Starts a conversation with the model. With terminal input and print

        Args:
            message_history (message_history): The conversation history.
            update_history (bool): Flag to update the conversation history.
            temperature (float): Control the randomness of the output.
        """
        print(self.get_completion(update_history=update_history, temperature=temperature))
        while True:
            user_input = input()
            if user_input == '\\break':
                break
            print(self.get_completion(message=user_input, update_history=update_history, temperature=temperature))
   
    def update_messages(self):
        self.messages = self.headers + self.body
        self.update_tokens()
        return self.messages

    def update_tokens(self):
        self.head_tokens = self.get_token_for_message(self.headers,model_name=self.model)
        self.body_tokens = self.get_token_for_message(self.body,model_name=self.model)
        self.tokens =self.get_token_for_message(self.messages,model_name=self.model)
        return self.tokens,self.head_tokens,self.body_tokens

    def calibrate(self,MAX_TOKEN=None): # We have to improve this to include the summarization stuff
        """
        self.calibrates the message history. Right now it does by comparing the token of the whole message with the MAX_TOKEN and remove the top message of from the body
        """
        if MAX_TOKEN is None:
            MAX_TOKEN = self.MAX_TOKEN
        self.update_tokens()
        if self.head_tokens >= MAX_TOKEN:
            raise Exception("HEAD TOKENS are greater than MAX TOKEN: Try reducing the Head prompts or increase MAX TOKEN")
        if self.tokens >= MAX_TOKEN:
            del self.body[0]
            self.update_messages()
            self.calibrate()
        
    def add(self, role, content, to_head=False):
        """
        Adds a message to the message history.

        Args:
            role (str): The role of the message (user, assistant, etc.).
            content (str): The content of the message.
            to_head (bool): Specifies whether the message should be appended to the headers list.
                            If False, it will be appended to the body list.
        
        Returns:
            dict: The last message in the message history.
        """
        if to_head:
            self.headers.append({"role": role, "content": content})
        else:
            self.body.append({"role": role, "content": content})

        self.update_messages()
        return self.messages[-1]
    
    def add_user(self, content, to_head=False):
        """
        Adds a user message to the message history.

        Args:
            content (str): The content of the user message.
            to_head (bool): Specifies whether the message should be appended to the headers list.
                            If False, it will be appended to the body list.
        
        Returns:
            dict: The last message in the message history.
        """
        self.add("user", content, to_head)
        return self.messages[-1]
    
    def add_assistant(self, content, to_head=False):
        """
        Adds an assistant message to the message history.

        Args:
            content (str): The content of the assistant message.
            to_head (bool): Specifies whether the message should be appended to the headers list.
                            If False, it will be appended to the body list.
        
        Returns:
            dict: The last message in the message history.
        """
        self.add("assistant", content, to_head)
        return self.messages[-1]
    
    def add_system(self, content, to_head=False):
        """
        Adds a system message to the message history.

        Args:
            content (str): The content of the system message.
            to_head (bool): Specifies whether the message should be appended to the headers list.
                            If False, it will be appended to the body list.
        
        Returns:
            dict: The last message in the message history.
        """
        self.add("system", content, to_head)
        return self.messages[-1]
        
    def append(self, content_list):
        """
        Appends a list of messages to the message history.

        Args:
            content_list (list): List of messages to be appended.
        """
        self.messages.extend(content_list)
       
    
    def get_last_message(self):
        """
        Returns the last message in the message history.

        Returns:
            dict: The last message in the message history.
        """
        return self.messages[-1]

    def get_token_for_message(self,messages, model_name="gpt-3.5-turbo-0613"):
        """Returns the number of tokens used by a list of messages."""
        
        try:
            encoding = tiktoken.encoding_for_model(model_name)
        except KeyError:
            encoding = tiktoken.get_encoding("cl100k_base")
        if model_name == "gpt-3.5-turbo-0613":  # note: future models may deviate from this
            num_tokens = 0
            for message in messages:
                num_tokens += 4  # every message follows <im_start>{role/name}\n{content}<im_end>\n
                for key, value in message.items():
                    num_tokens += len(encoding.encode(value))
                    if key == "name":  # if there's a name, the role is omitted
                        num_tokens += -1  # role is always required and always 1 token
            num_tokens += 2  # every reply is primed with <im_start>assistant
            return num_tokens
        else:
            raise NotImplementedError(f"""num_tokens_from_messages() is not presently implemented for model {model}.
        See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.""")

